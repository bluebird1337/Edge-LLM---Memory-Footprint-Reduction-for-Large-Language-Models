{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Pipeline\n",
    "\n",
    "Evaluation the performance of LLM over question answering task, focus on commonsense Reasoning ability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the required library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4feeb05549d04872ac8eaad2d4c984f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U trl transformers accelerate git+https://github.com/huggingface/peft.git\n",
    "!pip install -q datasets bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "In this section we will load the [Llama 7B model](https://huggingface.co/meta-llama/Llama-2-7b-hf), quantize it in 4bit and try to inference on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load peft config for pre-trained checkpoint etc.\n",
    "peft_model_id = \"/media/asdw/130a808e-ec26-42a1-93ab-42b857d97bd4/capstone_files/llama3-8b-qa-med-12k-r64-a16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85520fd08254ef482248cae9c1d88be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peft model loaded\n"
     ]
    }
   ],
   "source": [
    "# Load Model with PEFT adapter\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  # device_map=\"auto\",\n",
    "  # torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(peft_model_id)\n",
    "\n",
    "print(\"Peft model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "we will use the timdettmers/openassistant-guanaco dataset for our evaluation.\n",
    "\n",
    "The dataset can be found [here](https://huggingface.co/datasets/timdettmers/openassistant-guanaco/tree/831dabac2283d99420cda0b673d7a2a43849f17a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"/home/asdw/amazon-llm/code/CKM/dataset/medquad_instruct_test.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt_result(text):\n",
    "    parts = text.split(\"<s>[INST]\")\n",
    "    prompt = parts[0].split(\"[/INST] \\\\n\")[1].strip()\n",
    "    # parts = text.split(\"### Assistant:\")\n",
    "    # prompt = parts[0].split(\"### Human:\")[1].strip()\n",
    "\n",
    "    result = parts[1].strip()\n",
    "    return prompt, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_prompt_result(text):\n",
    "    # Split the input text based on the instruction delimiter\n",
    "    parts = text.split(\"<s>[INST]\")\n",
    "    \n",
    "    if len(parts) < 2:\n",
    "        raise ValueError(\"Invalid format: Missing '<s>[INST]' delimiter\")\n",
    "\n",
    "    # Split the second part to isolate the instruction and the following text\n",
    "    instruction_and_result = parts[1].split(\"[/INST] \\\\n\")\n",
    "\n",
    "    if len(instruction_and_result) < 2:\n",
    "        raise ValueError(\"Invalid format: Missing '[/INST] \\\\n' delimiter\")\n",
    "\n",
    "    # Extract the question (instruction part)\n",
    "    question = instruction_and_result[0].strip()\n",
    "\n",
    "    # Extract the result (remaining part)\n",
    "    result = instruction_and_result[1].strip()\n",
    "    \n",
    "    return question, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>What causes Madelung disease? The exact underl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>3MC syndrome is a disorder characterized by un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>What are the signs and symptoms of Buschke Oll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>Certain factors affect prognosis (chance of re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>Benign chronic pemphigus, often called Hailey-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>How is Pelizaeus-Merzbacher disease inherited?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>Phenylketonuria (PKU) is a genetic disorder in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>Mucolipidosis III alpha/beta is a slowly progr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>If you have chronic hepatitis C, you should do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>Sepsis is a serious illness. It happens when y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prompt  \\\n",
       "0    Answer the question truthfully, you are a medi...   \n",
       "1    Answer the question truthfully, you are a medi...   \n",
       "2    Answer the question truthfully, you are a medi...   \n",
       "3    Answer the question truthfully, you are a medi...   \n",
       "4    Answer the question truthfully, you are a medi...   \n",
       "..                                                 ...   \n",
       "195  Answer the question truthfully, you are a medi...   \n",
       "196  Answer the question truthfully, you are a medi...   \n",
       "197  Answer the question truthfully, you are a medi...   \n",
       "198  Answer the question truthfully, you are a medi...   \n",
       "199  Answer the question truthfully, you are a medi...   \n",
       "\n",
       "                                                result  \n",
       "0    What causes Madelung disease? The exact underl...  \n",
       "1    3MC syndrome is a disorder characterized by un...  \n",
       "2    What are the signs and symptoms of Buschke Oll...  \n",
       "3    Certain factors affect prognosis (chance of re...  \n",
       "4    Benign chronic pemphigus, often called Hailey-...  \n",
       "..                                                 ...  \n",
       "195  How is Pelizaeus-Merzbacher disease inherited?...  \n",
       "196  Phenylketonuria (PKU) is a genetic disorder in...  \n",
       "197  Mucolipidosis III alpha/beta is a slowly progr...  \n",
       "198  If you have chronic hepatitis C, you should do...  \n",
       "199  Sepsis is a serious illness. It happens when y...  \n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the function to the 'text' column\n",
    "eval_df = eval_dataset.to_pandas()\n",
    "eval_df[['prompt', 'result']] = eval_df['text'].apply(extract_prompt_result).apply(pd.Series)\n",
    "\n",
    "# Display the new DataFrame\n",
    "eval_df[['prompt', 'result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195.175\n",
      "231.4525345589649\n"
     ]
    }
   ],
   "source": [
    "tmp = eval_df.copy()\n",
    "\n",
    "tmp['result_len'] = len(str(tmp['result']))\n",
    "\n",
    "\n",
    "print(tmp['result'].apply(lambda x: len(x.split())).mean())\n",
    "print(tmp['result'].apply(lambda x: len(x.split())).std())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt):\n",
    "    formatted_prompt = (\n",
    "    # f\"A chat between a curious human and an artificial intelligence assistant.\"\n",
    "    # f\"The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\"\n",
    "    # f\"### Human: {prompt} ### Assistant:\"\n",
    "    f\"Answer the question truthfully, you are a medical professional. This is the question: {prompt}\"\n",
    ")\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=128,\n",
    "                         do_sample=True,\n",
    "                         temperature=0.9,\n",
    "                         top_k=50,\n",
    "                         top_p=0.9)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)[len(prompt):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"I want to start doing astrophotography as a hobby, any suggestions what could i do? Explain in 50 words.\"\n",
    "formatted_prompt = (\n",
    "    # f\"A chat between a curious human and an artificial intelligence assistant.\"\n",
    "    f\"The assistant gives helpful, detailed, and polite answers to the user's questions.\\n\"\n",
    "    f\"### Human: {prompt} ### Assistant:\"\n",
    ")\n",
    "# inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "# outputs = model.generate(inputs=inputs.input_ids, max_new_tokens=128)\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How does the Cypress test runner compare to the Selenium IDE in terms of usability and functionality?\n"
     ]
    }
   ],
   "source": [
    "rand_idx = randint(0, len(eval_df))\n",
    "\n",
    "prompt = eval_df['prompt'][rand_idx]\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sional. This is the question: What are the symptoms of Mucopolysaccharidosis type VI ? [/] \\n What are the signs and symptoms of Mucopolysaccharidosis type VI? The Human Phenotype Ontology provides the following list of signs and symptoms for Mucopolysaccharidosis type VI. If the information is available, the table below includes how often the symptom is seen in people with this condition. You can use the MedlinePlus Medical Dictionary to look up the definitions for these medical terms. Signs and Symptoms Approximate number of patients (when available) Abnormality of the teeth 90% Abnormality of the voice \n"
     ]
    }
   ],
   "source": [
    "res = generate(\"What are the symptoms of Mucopolysaccharidosis type VI ?\")\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e answers to the user's questions.\n",
      "### Human:  What are the symptoms of Loeys-Dietz syndrome ? ### Assistant: The symptoms of LMNA-related Loeys-Dietz syndrome can vary depending on the individual, but they can include:\n",
      "\n",
      "1.  Joint hypermobility, which is excessive range of motion in the joints.\n",
      "\n",
      "2.  Skin that is easily stretched and has a \"tent-like\" appearance.\n",
      "\n",
      "3.  Abnormalities of the spine and other bones, such as curvature, deformities, and weakness.\n",
      "\n",
      "4.  Heart and blood vessel problems, such as aortic root dilatation,\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "prompt = \" What are the symptoms of Loeys-Dietz syndrome ?\"\n",
    "res = generate(prompt)\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>prompt</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;[INST] Answer the question truthfully, you ...</td>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>What causes Madelung disease? The exact underl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;[INST] Answer the question truthfully, you ...</td>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>3MC syndrome is a disorder characterized by un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;s&gt;[INST] Answer the question truthfully, you ...</td>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>What are the signs and symptoms of Buschke Oll...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;s&gt;[INST] Answer the question truthfully, you ...</td>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>Certain factors affect prognosis (chance of re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;s&gt;[INST] Answer the question truthfully, you ...</td>\n",
       "      <td>Answer the question truthfully, you are a medi...</td>\n",
       "      <td>Benign chronic pemphigus, often called Hailey-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  <s>[INST] Answer the question truthfully, you ...   \n",
       "1  <s>[INST] Answer the question truthfully, you ...   \n",
       "2  <s>[INST] Answer the question truthfully, you ...   \n",
       "3  <s>[INST] Answer the question truthfully, you ...   \n",
       "4  <s>[INST] Answer the question truthfully, you ...   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  Answer the question truthfully, you are a medi...   \n",
       "1  Answer the question truthfully, you are a medi...   \n",
       "2  Answer the question truthfully, you are a medi...   \n",
       "3  Answer the question truthfully, you are a medi...   \n",
       "4  Answer the question truthfully, you are a medi...   \n",
       "\n",
       "                                              result  \n",
       "0  What causes Madelung disease? The exact underl...  \n",
       "1  3MC syndrome is a disorder characterized by un...  \n",
       "2  What are the signs and symptoms of Buschke Oll...  \n",
       "3  Certain factors affect prognosis (chance of re...  \n",
       "4  Benign chronic pemphigus, often called Hailey-...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'prompt', 'result'],\n",
       "    num_rows: 200\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "eval_dataset = Dataset.from_dict(eval_df)\n",
    "eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(sample):\n",
    "    # Convert the list to a PyTorch tensor\n",
    "    formatted_prompt = (\n",
    "    # f\"A chat between a curious human and an artificial intelligence assistant.\"\n",
    "    f\"Answer the question truthfully, you are a medical professional. This is the question: {sample['prompt']}\"\n",
    "\n",
    ")\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate outputs\n",
    "    outputs = model.generate(**inputs,\n",
    "                         max_new_tokens=128,\n",
    "                         do_sample=True,\n",
    "                         temperature=0.9,\n",
    "                         top_k=50,\n",
    "                         top_p=0.9)\n",
    "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)[len(sample['prompt']):]\n",
    "\n",
    "    labels = sample['result']\n",
    "\n",
    "    # Some simple post-processing\n",
    "    return prediction, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_rows = len(eval_dataset)\n",
    "random_indices = random.sample(range(total_rows), 200) # Random select 2 entries for evaluate\n",
    "test_samples = eval_dataset.select(random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  0%|          | 1/200 [02:13<7:23:46, 133.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  1%|          | 2/200 [04:28<7:22:23, 134.06s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 3/200 [06:42<7:20:30, 134.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▏         | 4/200 [08:57<7:19:36, 134.58s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  2%|▎         | 5/200 [11:12<7:17:43, 134.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  3%|▎         | 6/200 [13:26<7:14:44, 134.46s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▎         | 7/200 [15:40<7:11:46, 134.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 8/200 [17:54<7:09:19, 134.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  4%|▍         | 9/200 [20:08<7:06:54, 134.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  5%|▌         | 10/200 [22:22<7:04:38, 134.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 11/200 [24:36<7:02:19, 134.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▌         | 12/200 [26:50<7:00:01, 134.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  6%|▋         | 13/200 [29:07<7:00:37, 134.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  7%|▋         | 14/200 [31:23<6:59:04, 135.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 15/200 [33:39<6:57:41, 135.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 16/200 [35:55<6:56:12, 135.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  8%|▊         | 17/200 [38:10<6:53:36, 135.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "  9%|▉         | 18/200 [40:25<6:50:48, 135.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|▉         | 19/200 [42:41<6:48:20, 135.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 20/200 [44:55<6:45:29, 135.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 10%|█         | 21/200 [47:10<6:42:36, 134.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 11%|█         | 22/200 [49:25<6:40:24, 134.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 23/200 [51:40<6:38:20, 135.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▏        | 24/200 [53:55<6:36:24, 135.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 12%|█▎        | 25/200 [56:09<6:33:15, 134.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 13%|█▎        | 26/200 [58:23<6:30:18, 134.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▎        | 27/200 [1:00:39<6:28:34, 134.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 28/200 [1:02:53<6:26:24, 134.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 14%|█▍        | 29/200 [1:05:08<6:24:10, 134.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 15%|█▌        | 30/200 [1:07:23<6:21:27, 134.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 31/200 [1:09:37<6:18:48, 134.49s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▌        | 32/200 [1:11:51<6:16:47, 134.57s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 16%|█▋        | 33/200 [1:14:08<6:16:38, 135.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 17%|█▋        | 34/200 [1:16:25<6:15:25, 135.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 35/200 [1:18:39<6:11:50, 135.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 36/200 [1:20:53<6:08:49, 134.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 18%|█▊        | 37/200 [1:23:07<6:05:48, 134.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 19%|█▉        | 38/200 [1:25:22<6:03:12, 134.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|█▉        | 39/200 [1:27:36<6:00:51, 134.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 40/200 [1:29:50<5:58:14, 134.34s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 20%|██        | 41/200 [1:32:04<5:55:35, 134.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 21%|██        | 42/200 [1:34:18<5:53:14, 134.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 43/200 [1:36:36<5:53:52, 135.24s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▏       | 44/200 [1:38:54<5:53:44, 136.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 22%|██▎       | 45/200 [1:41:11<5:52:05, 136.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 23%|██▎       | 46/200 [1:43:25<5:48:46, 135.88s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▎       | 47/200 [1:45:42<5:46:43, 135.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 48/200 [1:47:58<5:45:03, 136.21s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 24%|██▍       | 49/200 [1:50:15<5:42:49, 136.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 25%|██▌       | 50/200 [1:52:31<5:40:34, 136.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 51/200 [1:54:47<5:38:16, 136.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▌       | 52/200 [1:57:03<5:36:00, 136.22s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 26%|██▋       | 53/200 [1:59:18<5:32:29, 135.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 27%|██▋       | 54/200 [2:01:35<5:31:29, 136.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 55/200 [2:03:49<5:27:41, 135.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 56/200 [2:06:04<5:24:21, 135.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 28%|██▊       | 57/200 [2:08:18<5:21:54, 135.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 29%|██▉       | 58/200 [2:10:34<5:19:42, 135.09s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|██▉       | 59/200 [2:12:48<5:17:07, 134.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 60/200 [2:15:02<5:14:17, 134.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 30%|███       | 61/200 [2:17:16<5:11:38, 134.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 31%|███       | 62/200 [2:19:30<5:09:04, 134.38s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 63/200 [2:21:45<5:06:56, 134.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▏      | 64/200 [2:24:02<5:06:14, 135.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 32%|███▎      | 65/200 [2:26:19<5:05:28, 135.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 33%|███▎      | 66/200 [2:28:34<5:02:30, 135.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▎      | 67/200 [2:30:49<4:59:52, 135.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 68/200 [2:33:05<4:58:12, 135.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 34%|███▍      | 69/200 [2:35:23<4:57:31, 136.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 35%|███▌      | 70/200 [2:37:37<4:54:17, 135.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 71/200 [2:39:53<4:51:56, 135.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▌      | 72/200 [2:42:08<4:48:50, 135.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 36%|███▋      | 73/200 [2:44:24<4:47:23, 135.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 37%|███▋      | 74/200 [2:46:39<4:44:27, 135.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 75/200 [2:48:53<4:41:31, 135.14s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 76/200 [2:51:08<4:38:53, 134.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 38%|███▊      | 77/200 [2:53:22<4:36:14, 134.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 39%|███▉      | 78/200 [2:55:36<4:33:42, 134.61s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|███▉      | 79/200 [2:57:50<4:31:05, 134.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 80/200 [3:00:05<4:28:55, 134.47s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 40%|████      | 81/200 [3:02:20<4:26:50, 134.54s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 41%|████      | 82/200 [3:04:35<4:24:53, 134.69s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 83/200 [3:06:49<4:22:35, 134.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▏     | 84/200 [3:09:04<4:20:25, 134.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 42%|████▎     | 85/200 [3:11:19<4:18:17, 134.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 43%|████▎     | 86/200 [3:13:34<4:16:19, 134.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▎     | 87/200 [3:15:49<4:14:09, 134.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 88/200 [3:18:04<4:11:48, 134.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 44%|████▍     | 89/200 [3:20:19<4:09:41, 134.97s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 45%|████▌     | 90/200 [3:22:34<4:07:32, 135.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 91/200 [3:24:49<4:05:17, 135.02s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▌     | 92/200 [3:27:04<4:02:58, 134.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 46%|████▋     | 93/200 [3:29:19<4:00:42, 134.98s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 47%|████▋     | 94/200 [3:31:34<3:58:29, 134.99s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 95/200 [3:33:49<3:56:08, 134.94s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 96/200 [3:36:06<3:55:06, 135.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 48%|████▊     | 97/200 [3:38:23<3:53:07, 135.80s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 49%|████▉     | 98/200 [3:40:38<3:50:26, 135.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|████▉     | 99/200 [3:42:52<3:47:42, 135.27s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 100/200 [3:45:10<3:46:52, 136.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 50%|█████     | 101/200 [3:47:28<3:45:18, 136.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 51%|█████     | 102/200 [3:49:43<3:42:30, 136.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 103/200 [3:51:58<3:39:35, 135.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▏    | 104/200 [3:53:06<3:04:50, 115.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 52%|█████▎    | 105/200 [3:55:21<3:12:12, 121.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 53%|█████▎    | 106/200 [3:57:37<3:16:52, 125.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▎    | 107/200 [3:59:52<3:19:05, 128.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 54%|█████▍    | 108/200 [4:02:08<3:20:18, 130.64s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▍    | 109/200 [4:04:24<3:20:44, 132.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 55%|█████▌    | 110/200 [4:06:40<3:19:58, 133.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 111/200 [4:08:58<3:19:49, 134.72s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▌    | 112/200 [4:11:14<3:18:04, 135.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 56%|█████▋    | 113/200 [4:13:31<3:16:52, 135.77s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▋    | 114/200 [4:15:47<3:14:49, 135.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 57%|█████▊    | 115/200 [4:18:04<3:12:53, 136.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 116/200 [4:20:19<3:10:05, 135.78s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 58%|█████▊    | 117/200 [4:22:33<3:07:19, 135.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 59%|█████▉    | 118/200 [4:24:48<3:04:50, 135.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|█████▉    | 119/200 [4:27:03<3:02:25, 135.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 120/200 [4:29:19<3:00:15, 135.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 60%|██████    | 121/200 [4:31:34<2:58:18, 135.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 61%|██████    | 122/200 [4:33:51<2:56:26, 135.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 123/200 [4:35:21<2:36:43, 122.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▏   | 124/200 [4:37:37<2:39:50, 126.18s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 62%|██████▎   | 125/200 [4:39:52<2:40:57, 128.76s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 63%|██████▎   | 126/200 [4:42:06<2:41:01, 130.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▎   | 127/200 [4:44:21<2:40:24, 131.85s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 128/200 [4:46:37<2:39:38, 133.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 64%|██████▍   | 129/200 [4:48:53<2:38:30, 133.95s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 65%|██████▌   | 130/200 [4:51:10<2:37:12, 134.75s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 131/200 [4:53:27<2:35:48, 135.48s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▌   | 132/200 [4:55:44<2:33:54, 135.79s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 66%|██████▋   | 133/200 [4:57:58<2:31:13, 135.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 67%|██████▋   | 134/200 [5:00:15<2:29:26, 135.86s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 135/200 [5:02:31<2:27:14, 135.92s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 136/200 [5:04:47<2:25:08, 136.08s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 68%|██████▊   | 137/200 [5:07:04<2:23:02, 136.23s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 69%|██████▉   | 138/200 [5:09:21<2:20:58, 136.42s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|██████▉   | 139/200 [5:11:39<2:19:17, 137.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 140/200 [5:13:57<2:17:16, 137.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 70%|███████   | 141/200 [5:16:13<2:14:33, 136.84s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 71%|███████   | 142/200 [5:18:31<2:12:33, 137.12s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 143/200 [5:20:47<2:09:58, 136.81s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▏  | 144/200 [5:23:02<2:07:04, 136.15s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 72%|███████▎  | 145/200 [5:25:17<2:04:44, 136.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 73%|███████▎  | 146/200 [5:27:32<2:02:06, 135.68s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▎  | 147/200 [5:29:47<1:59:36, 135.41s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 148/200 [5:32:02<1:57:16, 135.32s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 74%|███████▍  | 149/200 [5:34:19<1:55:21, 135.71s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 75%|███████▌  | 150/200 [5:36:36<1:53:28, 136.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 151/200 [5:38:54<1:51:33, 136.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▌  | 152/200 [5:41:10<1:49:16, 136.60s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 76%|███████▋  | 153/200 [5:43:26<1:46:45, 136.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 77%|███████▋  | 154/200 [5:45:41<1:44:11, 135.90s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 155/200 [5:45:43<1:11:58, 95.96s/it] Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 156/200 [5:47:58<1:18:55, 107.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 78%|███████▊  | 157/200 [5:50:14<1:23:12, 116.10s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 79%|███████▉  | 158/200 [5:52:29<1:25:09, 121.65s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|███████▉  | 159/200 [5:54:43<1:25:47, 125.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 160/200 [5:56:59<1:25:45, 128.63s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 80%|████████  | 161/200 [5:59:14<1:24:51, 130.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 81%|████████  | 162/200 [6:01:30<1:23:36, 132.03s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 163/200 [6:03:45<1:22:00, 133.00s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▏ | 164/200 [6:06:03<1:20:47, 134.66s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 82%|████████▎ | 165/200 [6:08:20<1:18:50, 135.17s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 83%|████████▎ | 166/200 [6:10:38<1:17:02, 135.96s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▎ | 167/200 [6:12:54<1:14:54, 136.19s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 168/200 [6:15:09<1:12:23, 135.73s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 84%|████████▍ | 169/200 [6:17:23<1:09:55, 135.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 85%|████████▌ | 170/200 [6:19:38<1:07:31, 135.05s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 171/200 [6:21:53<1:05:18, 135.11s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▌ | 172/200 [6:24:09<1:03:06, 135.25s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 86%|████████▋ | 173/200 [6:26:24<1:00:48, 135.13s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 87%|████████▋ | 174/200 [6:28:39<58:34, 135.18s/it]  Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 175/200 [6:30:54<56:22, 135.28s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 176/200 [6:33:10<54:10, 135.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 88%|████████▊ | 177/200 [6:35:26<51:57, 135.53s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 89%|████████▉ | 178/200 [6:37:41<49:39, 135.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|████████▉ | 179/200 [6:39:57<47:27, 135.59s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 180/200 [6:42:12<45:08, 135.45s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 90%|█████████ | 181/200 [6:44:27<42:51, 135.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 91%|█████████ | 182/200 [6:46:43<40:36, 135.36s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 183/200 [6:48:58<38:21, 135.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▏| 184/200 [6:51:14<36:06, 135.43s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 92%|█████████▎| 185/200 [6:53:29<33:50, 135.35s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 93%|█████████▎| 186/200 [6:55:44<31:35, 135.40s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▎| 187/200 [6:58:03<29:31, 136.29s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 188/200 [7:00:21<27:22, 136.91s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 94%|█████████▍| 189/200 [7:02:39<25:07, 137.07s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 95%|█████████▌| 190/200 [7:04:54<22:45, 136.55s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 191/200 [7:07:09<20:25, 136.20s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▌| 192/200 [7:09:24<18:06, 135.83s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 96%|█████████▋| 193/200 [7:11:40<15:49, 135.67s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 97%|█████████▋| 194/200 [7:13:54<13:32, 135.39s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 195/200 [7:16:09<11:15, 135.16s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 196/200 [7:18:25<09:02, 135.52s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 98%|█████████▊| 197/200 [7:20:08<06:16, 125.62s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      " 99%|█████████▉| 198/200 [7:22:23<04:17, 128.56s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|█████████▉| 199/200 [7:24:39<02:10, 130.70s/it]Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
      "100%|██████████| 200/200 [7:26:54<00:00, 134.07s/it]\n"
     ]
    }
   ],
   "source": [
    "predictions, references = [] , []\n",
    "for sample in tqdm(test_samples):\n",
    "    p,l = evaluate_model(sample)\n",
    "    predictions.append(p)\n",
    "    references.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'prediction': predictions,\n",
    "    'references': references\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save DataFrame to CSV file\n",
    "csv_filename = '/home/asdw/amazon-llm/code/CKM/Evaluation/inference_llama3_result_med_12k_maxlen_512_r64_a16.csv'\n",
    "df.to_csv(csv_filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the evaluation result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 09:03:18.705207: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload data from CSV file\n",
    "import pandas as pd\n",
    "csv_filename = '/home/asdw/amazon-llm/code/CKM/Evaluation/inference_llama3_result_med_12k_maxlen_512_r64_a16.csv'\n",
    "loaded_df = pd.read_csv(csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the lists from the DataFrame\n",
    "loaded_predictions = loaded_df['prediction'].tolist()\n",
    "loaded_references = loaded_df['references'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metric\n",
    "bertscore  = evaluate.load(\"bertscore\")\n",
    "\n",
    "def get_bert_score(predictions_, references_): # compute mean score of each prediction-reference match\n",
    "  score = bertscore.compute(predictions=predictions_, references=references_, lang=\"en\")\n",
    "  BERT_names = [\"precision\", \"recall\"]\n",
    "  bert_dict = dict((bn,  np.mean(score[bn])) for bn in BERT_names)\n",
    "  bert_dict['f1'] = 2*(bert_dict['precision']*bert_dict['recall']) / (bert_dict['precision']+bert_dict['recall'])\n",
    "  return bert_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Precision: 86.551847%\n",
      "Recall: 85.274260%\n",
      "f1: 85.908304%\n"
     ]
    }
   ],
   "source": [
    "scores = get_bert_score(loaded_predictions, loaded_references)\n",
    "\n",
    "# print results\n",
    "print(f\"\\nPrecision: {scores['precision']* 100:2f}%\")\n",
    "print(f\"Recall: {scores['recall']* 100:2f}%\")\n",
    "print(f\"f1: {scores['f1']* 100:2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rogue1: 36.463975%\n",
      "rouge2: 17.431370%\n",
      "rougeL: 24.310043%\n",
      "rougeLsum: 25.005254%\n"
     ]
    }
   ],
   "source": [
    "# compute metric\n",
    "rogue = metric.compute(predictions=loaded_predictions, references=loaded_references, use_stemmer=True)\n",
    "\n",
    "# print results\n",
    "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
    "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
    "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
    "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLUERT Score\n",
    "\n",
    "https://ai.googleblog.com/2020/05/evaluating-natural-language-generation.html\n",
    "\n",
    "https://github.com/google-research/bleurt\n",
    "\n",
    "https://arxiv.org/abs/2004.04696\n",
    "\n",
    "> Can  capture semantic similarties between sentences\n",
    "\n",
    "Employ transfer learning:\n",
    "\n",
    "1. Use the contextual word representation of BERT\n",
    "\n",
    "2. To prevent to effect of domain shift and quality drift -> Instead of collecting human ratings, we use a collection of metrics and models from the literature (BLEU, BERT Score, ROUGE...)\n",
    "\n",
    "3. To scaled up the training example, the use random deletions / round-trip translation / random substitution... to get multiple scores out of smae metric\n",
    "\n",
    "4. Fine-tuning on human ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone https://github.com/google-research/bleurt.git\n",
    "# cd bleurt\n",
    "# pip install .\n",
    "# wget https://storage.googleapis.com/bleurt-oss-21/BLEURT-20.zip .\n",
    "# unzip BLEURT-20.zip\n",
    "# python -m bleurt.score_files \\\n",
    "#   -candidate_file=bleurt/test_data/candidates \\\n",
    "#   -reference_file=bleurt/test_data/references \\\n",
    "#   -bleurt_checkpoint=BLEURT-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "new_working_directory = \"/home/asdw/amazon-llm/code/vincent_dev/bleurt\"\n",
    "os.chdir(new_working_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bleurt import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: BLEURT-20/sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: BLEURT-20/sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n",
      "2024-05-21 09:05:48.495136: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5138 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:67:00.0, compute capability: 7.5\n",
      "2024-05-21 09:05:48.495731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9080 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"BLEURT-20\"\n",
    "scorer = score.BleurtScorer(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLUERT Score: 38.047747%\n"
     ]
    }
   ],
   "source": [
    "scores = scorer.score(references=loaded_predictions, candidates=loaded_references)\n",
    "print(f\"BLUERT Score: {np.mean(scores)* 100:2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Update the evaluation pipeline\n",
    "2. Package the model into callable API\n",
    "3. Create UI for demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ckm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
